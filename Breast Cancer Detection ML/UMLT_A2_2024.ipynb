{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Assignment 2: Classification\n",
    "# Using Machine Learning Tools CS3317\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this assignment, you will apply some popular machine learning techniques to the problem of classifying data from histological cell images for the diagnosis of malignant breast cancer. This will be presented as a practical scenario where you are approached by a client to solve a problem.  \n",
    "\n",
    "The main aims of this assignment are: \n",
    "\n",
    "- to use the best practice machine learning workflow for producing a solution to a client's problem;\n",
    "- to visualise data and determine the best pre-processing;\n",
    "- to create the necessary datasets for training and testing purposes;\n",
    "- to train and optimise a selection of models, then choose the best;\n",
    "- to obtain an unbiased measurement of the final model's performance;\n",
    "- to interpret results clearly and concisely.\n",
    "\n",
    "This assignment relates to the following ACS CBOK areas: abstraction, design, hardware and software, data and information, HCI and programming.\n",
    "\n",
    "## General instructions\n",
    "\n",
    "This assignment is divided into several tasks. Use the spaces provided in this notebook to answer the questions posed in each task. Note that some questions require writing a small amount of code, some require graphical results. \n",
    "\n",
    "**Do not** manually edit the data set file we have provided! For marking purposes, it's important that your code runs correctly on the original data file.\n",
    "\n",
    "Some of the parts of this assignment build on the workflow from the first assignment and that part of the course, and so less detailed instructions are provided for this, as you should be able to implement this workflow now without low-level guidance. A substantial portion of the marks for this assignment are associated with making the right choices and executing this workflow correctly and efficiently.\n",
    "\n",
    "This assignment can be solved using methods from [sklearn](https://scikit-learn.org/stable/index.html), [pandas](https://pandas.pydata.org/pandas-docs/stable/index.html), and [matplotlib](https://matplotlib.org/stable/index.html) as presented in the workshops. **Other libraries should not be used** (even though they might have nice functionality) and certain restrictions on sklearn functions will be made clear in the instruction text. You are expected to search and carefully read the documentation for functions that you use, to ensure you are using them correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scenario\n",
    "\n",
    "A client approaches you to solve a machine learning problem for them. They run a pathology lab that processes histological images for healthcare providers and they have created a product that measures the same features as in the *Wisconsin breast cancer data set* though using different acquisitions and processing methods. This makes their method much faster than existing ones, but it is also slightly noisier. The dataset contains measurements of several features that describe characteristics of cell nuclei present in digital images of breast tissue samples. These features include the mean, standard error, and \"worst\" (i.e., largest) values of measurements such as radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry, and fractal dimension. Each feature is represented by a numeric value, and each data point represents a sample from either a malignant (cancerous) or benign (non-cancerous) breast tissue.\n",
    "\n",
    "The client want to be able to diagnose *malignant* cancer (and distinguish them from *benign* growths) by employing machine learning techniques, and they have asked you to implement this for them.\n",
    "\n",
    "Their requirements are:\n",
    " 1) have at least a 95% probability of detecting malignant cancer when it is present;\n",
    " 2) have no more than 1 in 10 healthy cases (those with benign tumours) labelled as positive (malignant).\n",
    " \n",
    "They have hand-labelled 300 samples for you, which is all they have at the moment.\n",
    "\n",
    "Please follow the instructions below, which will vary in level of detail, as appropriate to the marks given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This code imports some libraries that you will need. \n",
    "# You should not need to modify it, though you are expected to make other imports later in your code.\n",
    "\n",
    "# Common imports\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import re\n",
    "\n",
    "# Plot setup\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "mpl.rc('axes', labelsize=7)\n",
    "mpl.rc('xtick', labelsize=6)\n",
    "mpl.rc('ytick', labelsize=6)\n",
    "mpl.rc('figure', dpi=240)\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step 1** [1 point]\n",
    "\n",
    "Load the dataset Do this from the csv file, `assignment2.csv`. Extract the feature names for use later on. The first column is our target and it contains the labels (benign and malignant). Note that we will be treating the _malignant_ case as our _positive_ case, as this is the standard convention in medicine.\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = ...\n",
    "\n",
    "feature_names = ...\n",
    "\n",
    "# The following code is used by the autograder\n",
    "# make sure your variable that contains the answer from this step is the one assigned to step1_1data\n",
    "step1_data = feature_names"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Step 2** [3 points]\n",
    "\n",
    "As this data is well curated by the client already, you do not need to worry about outliers, missing values or imputation in this case, but be aware that this is the exception, not the rule.\n",
    "\n",
    "To familiarise yourself with the nature and information contained in the data, display histograms for the data according to the following instructions:\n",
    " - Isolate each group of features (mean, standard error, and worst) into its own DataFrame.\n",
    " - you are provided with code to display histograms for each feature in the _mean_ group. On _each_ histogram the two classes displayed together in one plot \n",
    " - **repeat this** for the _standard error_ and _worst_ groups; \n",
    "\n",
    "**Based on the histograms and using the function *corr()* or *numpy corrcoef()* which do you think are the 3 strongest features for discriminating between the classes?**\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Separate the mean, standard error, and worst groups into separate dataframes\n",
    "mean_group = ...\n",
    "error_group = ...\n",
    "worst_group = ...\n",
    "\n",
    "#  You can comment out the plotting code before uploading it to gradescope for faster evaluation\n",
    "class_labels = ['benign', 'malignant'] \n",
    "fig = plt.figure(figsize=(12, 8))  \n",
    "\n",
    "# Plot the histograms for each feature in the mean group with both classes displayed together\n",
    "for i, feature in enumerate(mean_group.columns):  \n",
    "    plt.subplot(3, 4, i+1)  \n",
    "    plt.hist([mean_group[feature][data['label'] == 'benign'], mean_group[feature]\n",
    "             [data['label'] == 'malignant']], label=class_labels)  \n",
    "    plt.xlabel(\"Feature Value\")  \n",
    "    plt.ylabel(\"Frequency\")  \n",
    "    plt.legend() \n",
    "    plt.title(feature)  \n",
    "\n",
    "plt.tight_layout()  \n",
    "plt.show()  \n",
    "\n",
    "# ranked_features = [\"Feature1 name\", \"Feature2 name\", \"Feature3 name\"] \n",
    "ranked_features = ...\n",
    "\n",
    "# The following code is used by the autograder\n",
    "step2_data = ranked_features"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "**Step3** [3 points]\n",
    "\n",
    "Convert the labels into 0s and 1s so that benign is represented by 0 and malignant is represented by 1.\n",
    "\n",
    "Split the dataset into appropriate subsets for training, validation and test sets. You must choose the size of each subset. However, make sure that the proportion of the two classes is consistent across all datasets using the _stratify_ option, as used in workshops 5 and 6. Verify the size and label distribution in each dataset.\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert labels to 0 (benign) and 1 (malignant)\n",
    "\n",
    "# Split dataset into training, validation, and test sets\n",
    "\n",
    "X_train = ...\n",
    "y_train = ...\n",
    "X_test = ...\n",
    "y_test = ...\n",
    "X_val = ...\n",
    "y_val = ...\n",
    "\n",
    "# Print the size of each resulting subset\n",
    "print(\"Training set size:\", len(X_train))\n",
    "print(\"Validation set size:\", len(X_val))\n",
    "print(\"Test set size:\", len(X_test))\n",
    "\n",
    "# The following code is used by the autograder\n",
    "step3_data = [len(X_train), len(X_val) , len(X_test)]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step03\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step4** [4 points]\n",
    "\n",
    "Baseline measurements \n",
    "\n",
    "For our classification task we will consider **three simple baseline cases**:\n",
    "1) predicting all samples to be negative (class 1)\n",
    "2) predicting all samples to be positive (class 2)\n",
    "3) making a random prediction for each sample with equal probability for each class \n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# create arrays to hold the predicted labels for each baseline case\n",
    "baseline1_preds = ...\n",
    "baseline2_preds = ...\n",
    "baseline3_preds = ...\n",
    "\n",
    "# The following code is used by the autograder.\n",
    "step4_data = [baseline1_preds, baseline2_preds, baseline3_preds] "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step04\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step5** [2 points]\n",
    "\n",
    "Write a function that returns the following metrics for all baseline measures created in Step4, use the function and print the metrics:\n",
    "\n",
    " - recall\n",
    " - precision\n",
    " - auc\n",
    " - f1score\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "...\n",
    "...\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):   \n",
    "    rec = ...\n",
    "    prec = ...\n",
    "    auc = ...\n",
    "    f1 = ...\n",
    "\n",
    "    return rec, prec, auc, f1\n",
    "\n",
    "# The following code is used by the autograder.\n",
    "step5_data = calculate_metrics"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step6** [3 points]\n",
    "\n",
    "Create a function which takes in the true labels and the predictions and returns the following parameters:\n",
    "\n",
    "* Number of True Positives (TP)\n",
    "* Number of True Negatives (TN)\n",
    "* Number of False Positive (FP)\n",
    "* Number of False Negative (FN)\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function returns the number of TP, TN, FP, FN.\n",
    "def get_values(y_true, y_pred):\n",
    "    tp = ...\n",
    "    tn = ...\n",
    "    fp = ...\n",
    "    fn = ...\n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "# The following code is used by the autograder.\n",
    "step6_data = get_values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step06\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step7 [3 points]**\n",
    "\n",
    "Based on the above baseline tests and the client's requirements, **choose a performance metric** to use for evaluating/driving your machine learning methods.\n",
    "\n",
    "Choose the best metric of the five and assign to the variable below.\n",
    "* For AUC, use \"roc_auc\"\n",
    "* For recall, use \"recall\"\n",
    "* For precision, use \"precision\"\n",
    "* For F1 Score, use \"f1\"\n",
    "\n",
    "***NOTE:*** Not all the metrics are equally useful for meeting the requirements of the client, thereby, choose only the one most suitable metric.\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Choose the best metric of the five and assign to the variable below. The variable takes a string.\n",
    "chosen_metric = ...\n",
    "# The following code is used by the autograder\n",
    "step7_data = chosen_metric"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step07\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step8** [3 points]\n",
    "\n",
    "**Creating a SGD baseline**\n",
    "\n",
    "For a stronger baseline, **train the Stochastic Gradient Descent classifier (SGD) model** on the training data and evaluate it on the validation data (as seen in workshop 4). Use a Pipeline for this which considers the preprocessing and the SGD model. For this baseline case use the default settings for all the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Build a pre-processing pipeline that includes imputation (as even though we don't strictly need it here it is a good habit to always include it) and other appropriate pre-processing. Create another pipeline for the SGD model which has preprocessing pipeline as the pre-processing part. Train the model and evaluate it on the validation data.   \n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create pre-processing pipeline\n",
    "preproc_pl = ...\n",
    "\n",
    "# Create SGD model pipeline which has preproc_pl as the pre-processing part\n",
    "sgd_base = ...\n",
    "\n",
    "# fit the SGD pipeline on the training data\n",
    "\n",
    "# evaluate on the validation data\n",
    "\n",
    "# The following code is used by the autograder.\n",
    "step8_data = sgd_base"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step08\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step9** [2 points]\n",
    "\n",
    "On the validation data, calculate and print or display the **normalized** version of the confusion matrix.  Given the normalized confusion matrix, **what is the _probability_ that a sample from a person with a malignant tumour is given a result that they do not have cancer?  Which of the client's two criteria does this relate to, and is this baseline satisfying this criterion or not?**\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Put your confusion matrix here\n",
    "cmat = ...\n",
    "\n",
    "# Display or print cmat\n",
    "\n",
    "# Assign the required probability to the variable below\n",
    "required_prob = ...\n",
    "# Choose which of the client's two criteria does this relate to (1 or 2)\n",
    "criteria = ...\n",
    "# Does the baseline model satisfy this criterion? (\"YES\" or \"NO\")\n",
    "satisfy = ...\n",
    "# The following code is used by the autograder.\n",
    "step9_data = [cmat, required_prob , criteria , satisfy]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step09\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main classifier \n",
    "\n",
    "In this part, you need to try different models for classification, check their performance and determine the best for the present scenario.\n",
    "\n",
    "Follow best practice as much as possible here. You must make all the choices and decisions yourself, and strike a balance between computation time and performance.\n",
    "\n",
    "You can use any of the sklearn functions used in workshops 3, 4, 5 and 6. Other hyper-parameter optimisation functions apart from these cannot be used (even if they are good and can be part of best practice in other situations - for this assignment everyone should assume they only have very limited computation resources and limit themselves to these functions). Hint: Use **GridSearchCV** to solve these steps.\n",
    "\n",
    "**Display the performance of the different classifiers and the optimised hyperparameters.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step10** [3 point]\n",
    "\n",
    "**Train and optimise the hyperparameters** to give the best performance for a **KNN (K-Nearest Neighbour)** classifier. For this model consider and  evaluate the following parameters:\n",
    "\n",
    "* n_neighbors (with no value greater than 20)\n",
    "* weights\n",
    "* algorithm\n",
    "* metric\n",
    "* p (with no value greater than 20)\n",
    "\n",
    "Read the documentation about this model to find some proper ranges and ensure that you use the pipeline created in Step8 to train this model. How to decide what are the best setting of parameters? -> use the best metric identified in the **Step7**.\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write your solution here\n",
    "\n",
    "# Put the pipeline with the appropriate model \n",
    "knn_pl = ...\n",
    "\n",
    "\n",
    "\n",
    "# Use GridSearchCV here with cv=5\n",
    "knn_model = ...\n",
    "\n",
    "print(f'knn_best_parameters : {knn_model.best_params_}') \n",
    "\n",
    "# Return best parameters in a dictionary\n",
    "knn_best_parameters = ...\n",
    "\n",
    "# The following code is used by the autograder.\n",
    "step10_data = knn_model, knn_best_parameters, knn_pl"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step11** [2 points]\n",
    "\n",
    "**Train and optimise the hyperparameters** to give the best performance for a **Decision tree classifier** classifier. For this model consider and  evaluate the following parameters:\n",
    "\n",
    "* criterion\n",
    "* max_depth (with no value greater than 20)\n",
    "* min_samples_split (with no value greater than 20)\n",
    "* min_samples_leaf (with no value greater than 20)\n",
    "* max_features\n",
    "\n",
    "Read the documentation about this model to find some proper ranges and ensure that you use the pipeline created in Step8 to train this model. \n",
    "\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Put the pipeline with the appropriate model \n",
    "dt_pl = ...\n",
    "\n",
    "\n",
    "\n",
    "# Use GridSearchCV with cv=5\n",
    "dt_model = ...\n",
    "\n",
    "\n",
    "# Return best parameters in a dictionary\n",
    "dt_best_parameters = ...\n",
    "\n",
    "# The following code is used by the autograder.\n",
    "step11_data = dt_model,dt_best_parameters, dt_pl"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step12** [2 points]\n",
    "\n",
    "**Train and optimise the hyperparameters** to give the best performance for a **C-Support Vector Classification** classifier. For this model consider and  evaluate the following parameters:\n",
    "\n",
    "* C. Regularization parameter (with no value greater than 100)\n",
    "* kernel\n",
    "* gamma\n",
    "\n",
    "Read the documentation about this model to find some proper ranges and ensure that you use the pipeline created in Step8 to train this model. \n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Put the pipeline with the appropriate model \n",
    "svc_pl = ...\n",
    "\n",
    "\n",
    "\n",
    "# Use GridSearchCV with cv=5 \n",
    "svc_model = ...\n",
    "\n",
    "# Return best parameters in a dictionary\n",
    "svc_best_parameters = ...\n",
    "\n",
    "# The following code is used by the autograder.\n",
    "step12_data = svc_model,svc_best_parameters, svc_pl"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step13** [2 points]\n",
    "\n",
    "**Train and optimise the hyperparameters** to give the best performance for a **SGD classifier** classifier. For this model consider and  evaluate the following parameters:\n",
    "\n",
    "* loss \n",
    "* penalty\n",
    "* alpha (with no value greater than 100)\n",
    "* learning_rate\n",
    "* eta0 (with no value greater than 100)\n",
    "\n",
    "Read the documentation about this model to find some proper ranges and ensure that you use the pipeline created in Step8 to train this model. \n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Put the pipeline with the appropriate model \n",
    "sgd_pl = ...\n",
    "\n",
    "\n",
    "\n",
    "# Use GridSearchCV with cv=5\n",
    "sgd_model = ...\n",
    "\n",
    "# Return best parameters in a dictionary\n",
    "sgd_best_parameters = ...\n",
    "\n",
    "# The following code is used by the autograder.\n",
    "step13_data = sgd_model,sgd_best_parameters, sgd_pl"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step13\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step14** [2 points]\n",
    "\n",
    "Considering the previous results using **GridSearchCV** for each one of the main classifiers, retrieve the obtained score using the best parameters in each case. Also indicate which are the three **best** models. \n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assign the best score for each model \n",
    "knn_best_cv_scoring = ...\n",
    "dt_best_cv_scoring  = ...\n",
    "svc_best_cv_scoring = ...\n",
    "sgd_best_cv_scoring = ...\n",
    "\n",
    "'''\n",
    "What are the three best models so far? Assign a list of the three best models\n",
    "to the variable below (best to worst). The model options are \"knn\", \"dt\", \"svc\" or \"sgd\"\n",
    "'''\n",
    "best_models = ...\n",
    "\n",
    "# The following code is used by the autograder.\n",
    "step14_data = best_models,knn_best_cv_scoring,dt_best_cv_scoring,svc_best_cv_scoring,sgd_best_cv_scoring"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step14\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step15** [0 points]\n",
    "\n",
    "Once you have performed the search of the best parameters for each one of these models, you can display the general performance of each model using the next function **plot_compare_classifier_score**. An example plot using the function provided would look like the following:\n",
    "\n",
    "<center><img src=\"scoring_curves.jpg\" width=900 alt=\"Example loss curve plot\"></center>\n",
    "\n",
    "_Points:_ 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to check the performance of each parameter.\n",
    "def pooled_var(stds):\n",
    "    # https://en.wikipedia.org/wiki/Pooled_variance#Pooled_standard_deviation\n",
    "    n = 5 # size of each group\n",
    "    return np.sqrt(sum((n-1)*(stds**2))/ len(stds)*(n-1))\n",
    "\n",
    "# Function to create loss curves\n",
    "def plot_gridSearchCV_loss_curve(cv_results,grid_params,title):\n",
    "\n",
    "    df = pd.DataFrame(cv_results)\n",
    "    results = ['mean_test_score',\n",
    "               'mean_train_score',\n",
    "               'std_test_score',\n",
    "               'std_train_score']\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(grid_params),\n",
    "                             figsize = (5*len(grid_params), 7),\n",
    "                             sharey='row')\n",
    "    axes[0].set_ylabel(\"Score\", fontsize=25)\n",
    "\n",
    "\n",
    "    for idx, (param_name, param_range) in enumerate(grid_params.items()):\n",
    "        grouped_df = df.groupby(f'param_{param_name}')[results]\\\n",
    "            .agg({'mean_train_score': 'mean',\n",
    "                  'mean_test_score': 'mean',\n",
    "                  'std_train_score': pooled_var,\n",
    "                  'std_test_score': pooled_var})\n",
    "\n",
    "        previous_group = df.groupby(f'param_{param_name}')[results]\n",
    "        axes[idx].set_xlabel(param_name, fontsize=30)\n",
    "        axes[idx].set_ylim(0.0, 1.1)\n",
    "        lw = 2\n",
    "        axes[idx].plot(param_range, grouped_df['mean_train_score'], label=\"Training score\",\n",
    "                    color=\"darkorange\", lw=lw)\n",
    "        axes[idx].fill_between(param_range,grouped_df['mean_train_score'] - grouped_df['std_train_score'],\n",
    "                        grouped_df['mean_train_score'] + grouped_df['std_train_score'], alpha=0.2,\n",
    "                        color=\"darkorange\", lw=lw)\n",
    "        axes[idx].plot(param_range, grouped_df['mean_test_score'], label=\"Cross-validation score\",\n",
    "                    color=\"navy\", lw=lw)\n",
    "        axes[idx].fill_between(param_range, grouped_df['mean_test_score'] - grouped_df['std_test_score'],\n",
    "                        grouped_df['mean_test_score'] + grouped_df['std_test_score'], alpha=0.2,\n",
    "                        color=\"navy\", lw=lw)\n",
    "\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.suptitle(f'{title} Validation curves', fontsize=30)\n",
    "    fig.legend(handles, labels, loc=8, ncol=2, fontsize=20)\n",
    "\n",
    "    fig.subplots_adjust(bottom=0.25, top=0.85)\n",
    "    plt.show()\n",
    "    \n",
    "# Check the performance for each model (knn, dt, svc and sgd). Use plot_gridSearchCV_loss_curve() function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step16** [3 points]\n",
    "\n",
    "Having identified the best three models in Step 14, train them using their best parameters on the training data, and then make predictions on the validation set. Finally, calculate and display the following metrics for each case:\n",
    "\n",
    "* recall\n",
    "* precision\n",
    "* auc score\n",
    "* f1 score\n",
    "  \n",
    "To pass this step, non of the above metric should be below 0.8\n",
    "\n",
    "***HINT:*** Use the function you have created in Step5 to obtain the above metrics for each model.\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Assign the predictions made by each model to the variables below.\n",
    "'''\n",
    "predictions_model1 = ...\n",
    "predictions_model2 = ...\n",
    "precictions_model3 = ...\n",
    "\n",
    "'''\n",
    "HINT: Use the function you have created in Step5. The function returns a tuple\n",
    "containing the metrics recall ,  precision , auc score f1 score.\n",
    "'''\n",
    "metrics_model1 = ...\n",
    "metrics_model2 = ...\n",
    "metrics_model3 = ...\n",
    "\n",
    "# The following code is used by the autograder.\n",
    "step16_data = [(predictions_model1, predictions_model2, precictions_model3),(metrics_model1, metrics_model2, metrics_model3)]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step17** [4 points]\n",
    "\n",
    "**Final performance**\n",
    "\n",
    "Choose the best model of the top 3 you obtained in **Step14**. Calculate and display an unbiased performance measure that you can present to the client.\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Assign the training features, training labels that you would use for this step to the variables\n",
    "below.\n",
    "'''\n",
    "X_train_final = ...\n",
    "y_train_final = ...\n",
    "\n",
    "# Assign the best model to the variable below and train it\n",
    "final_model = ...\n",
    "\n",
    "\n",
    "# Assign the predictions made from your chosen best model for the unbiased estimate to the variable below.\n",
    "predictions_final_model = ...\n",
    "\n",
    "# Choose a performance metric based on the client's requirement and assign the result to the variable below.\n",
    "chosen_performance_metric_result = ...\n",
    "\n",
    "# The following code is used by the autograder\n",
    "step17_data = (predictions_final_model, chosen_performance_metric_result, X_train_final, y_train_final,final_model)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step17\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step18** [3 points]\n",
    "\n",
    "**Rank features**\n",
    "\n",
    "Although it is only possible to know the true usefulness of a feature when you've combined it with others in a machine learning method, it is still helpful to have some measure for how discriminative each feature is on its own.  One common method for doing this is to calculate a *T-score* (often used in statistics, and in the **LDA** machine learning method) for each feature. The formula for the *T-score* is \n",
    "\n",
    "$$\n",
    "  T_{score} =  \\frac{(mean(x_{2}) - mean(x_{1}))}{0.5(stddev(x_{2}) + stddev(x_{1}))}\n",
    "$$\n",
    "\n",
    "where $x_{1}$ and $x_{2}$ are the datasets corresponding to the two classes. Large values for the *T-score* (either positive or negative) indicate discriminative ability. Define a function which returns this *T-score*. For this process use the entire dataset.\n",
    "\n",
    "**Using the defined function, calculate the *T-score* for each feature and obtain the best 4 features according to this score.**\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Implement this function to calculate T score\n",
    "def calculate_t_score(x1, x2):\n",
    "    ...\n",
    "\n",
    "# Split the training data into two datasets corresponding to the two classes\n",
    "\n",
    "# Calculate the T score for the feature\n",
    "\n",
    "# Assign the top 4 features (in order) to this variable. It should be a list containing 4 feature names (strings).\n",
    "best_four_features = ...\n",
    "\n",
    "# The following code is used by the autograder.\n",
    "step18_data = best_four_features.copy()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step18\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step19 [2 points]**\n",
    "\n",
    "You can visualize the results given by your model using a decision boundary. For this step, use the best two features (based on the previos step) and create a decision boundary plot. For this, use the best model obtained from Step17. The visualization of decision boundaries should be done using `DecisionBoundaryDisplay` from sklearn.inspection.\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# Assign the name of the best feature obtained in step18 to the variable below. (string)\n",
    "feature_one = ...\n",
    "\n",
    "# Assign the name of the second best feature obtained in step18 to the variable below. (string)\n",
    "feature_two = ...\n",
    "\n",
    "# Assign the training dataset that you would want to use for this step to the variable below\n",
    "data2d = ...\n",
    "\n",
    "'''\n",
    "Check the decumentation of DecisionBoundaryDisplay in sklearn from \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.inspection.DecisionBoundaryDisplay.html.\n",
    "\n",
    "Use DecisionBoundaryDisplay.from_estimator(...) and assign the instance to the variable\n",
    "below.\n",
    "'''\n",
    "disp_step19 = ...\n",
    "\n",
    "# Plotting the data points. Use this to create the scatter plot\n",
    "disp_step19.ax_.scatter(X_train[feature_one], X_train[feature_two],\n",
    "                        c=y_train, edgecolor=\"k\",\n",
    "                        cmap=plt.cm.coolwarm)\n",
    "plt.xlim(-0.3, 0.3)\n",
    "plt.title(f\"Decision surface for tree trained on {feature_one} and {feature_two}\")\n",
    "plt.show()\n",
    "\n",
    "# The following line of code is used by the autograder\n",
    "step19_data = feature_one, feature_two, data2d"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step19\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Round\n",
    "\n",
    "After presenting your initial results to the client they come back to you and say that they have done some financial analysis and it would save them a lot of time and money if they did not have to analyse every cell, which is needed to get the \"worst\" features. Instead, they can quickly get accurate estimates for the \"mean\" and \"standard error\" features from a much smaller, randomly selected set of cells.\n",
    "\n",
    "They ask you to **give them a performance estimate for the same problem, but without using any of the \"worst\" features.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step20** [2 points]\n",
    "\n",
    "Create a new dataset according to the specifications of the client in round 2. After that, split the data appropriately.\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assign the data to each variable appropriately\n",
    "X_train_r2 = ...\n",
    "y_train_r2 = ...\n",
    "X_test_r2 = ...\n",
    "y_test_r2 = ...\n",
    "X_val_r2 = ...\n",
    "y_val_r2 = ...\n",
    "\n",
    "# Print the size of each resulting subset\n",
    "print(\"Training set size (second round):\", len(X_train_r2))\n",
    "print(\"Validation set size (second round):\", len(X_val_r2))\n",
    "print(\"Test set size (second round):\", len(X_test_r2))\n",
    "\n",
    "# The following code is used by the autograder, don't change it\n",
    "step20_data = {\n",
    "    \"X_train_r2_shape\": X_train_r2.shape,\n",
    "    \"X_val_r2_shape\": X_val_r2.shape,\n",
    "    \"X_test_r2_shape\": X_test_r2.shape,\n",
    "    \"feature_names\": list(X_train_r2.columns),\n",
    "    \"label_counts_train\": y_train_r2.value_counts().to_dict(),\n",
    "    \"label_counts_val\": y_val_r2.value_counts().to_dict(),\n",
    "    \"label_counts_test\": y_test_r2.value_counts().to_dict()\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step21** [3 points]\n",
    "\n",
    "Train the best model obtained in the first round on the new dataset and find the best parameters.\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Assign the GridSearchCV instance you have obtained in this step to the variable below.\n",
    "final_model_r2 = ...\n",
    "\n",
    "print(f'best_parameters_r2: {final_model_r2.best_params_}')\n",
    "# Assign the best parameter dictionary to the variable below.\n",
    "best_parameters_r2 = ...\n",
    "\n",
    "# 1. Predict on the validation set\n",
    "y_pred_val = ...\n",
    "\n",
    "# 2. Calculate performance metrics\n",
    "accuracy_val = ...\n",
    "recall_val = ...\n",
    "precision_val = ...\n",
    "f1_val = ...\n",
    "# If the model provides probability estimates of the positive class,\n",
    "# use them for AUC score calculation; else, use the predictions directly\n",
    "auc_val = ...\n",
    "\n",
    "\n",
    "\n",
    "# The following code is used by the autograder.\n",
    "step21_data = {\n",
    "    \"final_model_r2\": final_model_r2,\n",
    "    \"best_parameters_r2\": best_parameters_r2,\n",
    "    \"validation_metrics\": {\n",
    "        \"accuracy\": accuracy_val,\n",
    "        \"recall\": recall_val,\n",
    "        \"precision\": precision_val,\n",
    "        \"f1\": f1_val,\n",
    "        \"auc\": auc_val\n",
    "    },\n",
    "    \"validation_predictions\": y_pred_val\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step21\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step22** [3 points]\n",
    "\n",
    "Considering the best parameters identified in the previous step, train your best model using those parameters and calculate and display an unbiased performance measure.\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Assign the training features, training labels that you would use for this step to the variables\n",
    "below.\n",
    "''' \n",
    "X_train_final_r2 = ...\n",
    "y_train_final_r2 = ...\n",
    "\n",
    "# Assign the best model you have trained again in order to get an unbiased performance estimate.\n",
    "final_model_r2 = ...\n",
    "\n",
    "# Assign the predictions made from your chosen best model for the unbiased estimate to the variable below.\n",
    "predictions_final_model_r2 = ...\n",
    "\n",
    "# Choose a performance metric based on the client's requirement and assign the result to the variable below.\n",
    "performance_metric_result_r2 = ...\n",
    "\n",
    "# The following code is used by the autograder\n",
    "step22_data = {\n",
    "    \"predictions_final_model_r2\": predictions_final_model_r2,\n",
    "    \"performance_metric_result_r2\": performance_metric_result_r2\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step22\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step23** [2 points]\n",
    "\n",
    "For this step you need to compare the final performance obtained in round 1 with the performance given in round 2. Don't run your models again, use the results from step17 and step22.\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Final performance for 1st round\n",
    "final_performance_r1 = ...\n",
    "\n",
    "# Final performance for 2nd round\n",
    "final_performance_r2 = ...\n",
    "\n",
    "# What is your best model, \"r1\" or \"r2\". Put your answer in the variable final_answer (example: final_answer = \"r2\")\n",
    "final_answer = ...\n",
    "\n",
    "# The following code is used by the autograder\n",
    "step23_data = {\n",
    "    \"final_performance_r1\": final_performance_r1,\n",
    "    \"final_performance_r2\": final_performance_r2,\n",
    "    \"final_answer\": final_answer\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step23\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Step24** [3 points]\n",
    "\n",
    "Once you have gone through the rigorous process of testing and evaluating different models with various parameters and in diverse circumstances, you now have a robust model capable of generalizing well to unseen data.\n",
    "\n",
    "The client has provided data about a new patient in a table format, and is expecting you to make a prediction about the person's condition as soon as possible. The following table contains the information provided by the client.\n",
    "\n",
    "| Characteristic | Value |\n",
    "| --- | --- |\n",
    "| mean radius |13.970546 | \n",
    "| mean texture|15.660529 | \n",
    "| mean perimeter | 91.432976| \n",
    "| mean area |600.575775 | \n",
    "| mean smoothness |0.099883 | \n",
    "| mean compactness | NaN| \n",
    "| mean concavity | 0.07505| \n",
    "| mean concave points | 0.023223| \n",
    "| mean symmetry | 0.186472| \n",
    "| mean fractal dimension | 0.05446|\n",
    "| radius error | 0.321022 | \n",
    "| texture error |1.044633 | \n",
    "| perimeter error |2.324773 | \n",
    "| area error |31.333479 | \n",
    "| smoothness error | 0.005675| \n",
    "| compactness error |0.023401 | \n",
    "| concavity error |0.026742 | \n",
    "| concave points error |0.009808 | \n",
    "| symmetry error | 0.029254| \n",
    "| fractal dimension error |0.00325 |\n",
    "| worst radius |12.728135 | \n",
    "| worst texture | 23.476671| \n",
    "| worst perimeter |103.834522 | \n",
    "| worst area | 798.832368| \n",
    "| worst smoothness |0.130751 | \n",
    "| worst compactness |0.264469 | \n",
    "| worst concavity |0.247665 | \n",
    "| worst concave points | 0.100153| \n",
    "| worst symmetry |0.301228 | \n",
    "| worst fractal dimension |0.083051 |  \n",
    "\n",
    "Your goal is to inform the client whether the sample corresponds to a benign or malignant case. Provide the class and also the probability that the sample belongs to the malignant class.\n",
    "\n",
    "Note: If your best model corresponds to SGD using loss='perceptron', you can use decision_function instead of predict_proba to obtain the confidence score for your sample instead of the probability of belonging to the malignant class. Refer to the documentation of SGDClassifier for more information on this case.\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Put your new data in form of a dataframe\n",
    "new_sample = ...\n",
    "\n",
    "# Use the best model to predict. \n",
    "\n",
    "# Prediction of the class. You need to say if the sample is bening or malignant (\"malignant\" or \"benign\")\n",
    "prediction_sample = ...\n",
    "\n",
    "# Probability or confidence score of the sample belonging to the malignant class\n",
    "probability_sample = ...\n",
    "\n",
    "# Print your results\n",
    "print(\"The class for this sample is \", prediction_sample)\n",
    "\n",
    "# NOTE: in the case of SGD, Confidence score values will not be bounded within 0 and 1.\n",
    "print(\"The probability / confidence score of belonging to malignant class is \", probability_sample)\n",
    "\n",
    "# The following code is used by the autograder\n",
    "step24_data = {\n",
    "    \"prediction_sample\": prediction_sample,\n",
    "    \"probability_sample\": probability_sample\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "grader.check(\"step24\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
